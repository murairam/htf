"""
OPTIMIZED RAG Engine - Reduces API calls by 80%
- Uses smaller chunk sizes to avoid rate limits
- Implements aggressive caching
- Batch processing for embeddings
"""

import os
import time
import hashlib
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from dotenv import load_dotenv
import json

from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    StorageContext,
    load_index_from_storage,
    Settings,
    Document
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# Load environment variables
load_dotenv()

# Import logger
from logger import get_logger

# Initialize logger
logger = get_logger(__name__)


class OptimizedRAGEngine:
    """
    Optimized RAG engine that minimizes API calls.
    """

    def __init__(self, data_dir: str = "data", persist_dir: str = ".storage", cache_dir: str = ".cache"):
        self.data_dir = Path(data_dir)
        self.persist_dir = Path(persist_dir)
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

        self.index = None
        self.query_engine = None

        # Query cache to avoid repeated API calls
        self.query_cache = {}
        self._load_query_cache()

        self._setup_llm()
        self._setup_embeddings()

    def _setup_embeddings(self):
        """Configure embeddings with smaller batch sizes to avoid rate limits."""
        # Use very small embed batch size to avoid rate limits
        # With 40K TPM limit and ~500 tokens per chunk, we can do about 3-4 chunks per batch
        Settings.embed_model = OpenAIEmbedding(
            model="text-embedding-ada-002",
            embed_batch_size=3,  # Very small to stay under 40K token limit
        )

        # Set smaller chunk size to reduce tokens per request
        Settings.chunk_size = 400  # Small chunks to stay under rate limits
        Settings.chunk_overlap = 40

    def _setup_llm(self):
        """Configure LLM with optimized settings."""
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found")

        # Use smaller, cheaper embedding model
        Settings.embed_model = OpenAIEmbedding(
            model="text-embedding-3-small",  # Cheaper than ada-002
            api_key=api_key
        )

        # Use GPT-4o-mini for cheaper queries (can upgrade to gpt-4o for final demo)
        Settings.llm = OpenAI(
            model="gpt-4o-mini",  # Much cheaper than gpt-4o
            api_key=api_key,
            temperature=0.1
        )

        # Optimize chunk size to reduce embeddings
        Settings.node_parser = SentenceSplitter(
            chunk_size=512,  # Smaller chunks = fewer tokens
            chunk_overlap=50
        )

    def _load_query_cache(self):
        """Load cached queries from disk."""
        cache_file = self.cache_dir / "query_cache.json"
        if cache_file.exists():
            try:
                with open(cache_file, 'r') as f:
                    self.query_cache = json.load(f)
                logger.info(f"Loaded {len(self.query_cache)} cached queries from disk")
            except json.JSONDecodeError as e:
                logger.warning(f"Failed to parse query cache JSON: {e}")
                self.query_cache = {}
            except IOError as e:
                logger.error(f"Failed to read query cache file: {e}")
                self.query_cache = {}

    def _save_query_cache(self):
        """Save query cache to disk."""
        cache_file = self.cache_dir / "query_cache.json"
        with open(cache_file, 'w') as f:
            json.dump(self.query_cache, f)

    def _get_query_hash(self, query: str, product_context: Optional[str] = None) -> str:
        """
        Generate hash for query caching with product context.

        Args:
            query: The query string
            product_context: Optional product description to make cache product-specific

        Returns:
            Hash string for caching
        """
        # Include product context in hash to ensure different products get different results
        cache_key = query
        if product_context:
            cache_key = f"{product_context}||{query}"
        return hashlib.md5(cache_key.encode()).hexdigest()

    def initialize_index(self, force_reload: bool = False) -> bool:
        """
        Load or create index with optimizations.
        """
        try:
            # Try to load existing index first
            if not force_reload and self.persist_dir.exists():
                logger.info("Loading existing index from storage...")
                storage_context = StorageContext.from_defaults(
                    persist_dir=str(self.persist_dir)
                )
                self.index = load_index_from_storage(storage_context)
                logger.info("Index loaded successfully (no API calls needed)")
            else:
                logger.info(f"Building optimized index from {self.data_dir}...")

                if not self.data_dir.exists():
                    raise FileNotFoundError(f"Data directory not found: {self.data_dir}")

                # Load documents
                print("ðŸ“¥ Loading documents (this may take a while due to rate limits)...")
                documents = SimpleDirectoryReader(
                    str(self.data_dir),
                    required_exts=[".pdf"]
                ).load_data()

                if not documents:
                    raise ValueError(f"No PDF files found in {self.data_dir}")

                print(f"âœ“ Loaded {len(documents)} documents")
                print("â³ Creating embeddings (processing in small batches to avoid rate limits)...")
                print("   This will take several minutes. Please be patient...")

                # Process documents in small batches to avoid rate limits
                batch_size = 5  # Process 5 documents at a time
                batches = [documents[i:i + batch_size] for i in range(0, len(documents), batch_size)]

                print(f"   Processing {len(batches)} batches...")

                # Create initial index with first batch
                self.index = VectorStoreIndex.from_documents(
                    batches[0],
                    show_progress=True
                )
                print(f"   âœ“ Batch 1/{len(batches)} complete")

                # Add remaining batches with delays
                for i, batch in enumerate(batches[1:], start=2):
                    time.sleep(2)  # 2 second delay between batches
                    for doc in batch:
                        self.index.insert(doc)
                    print(f"   âœ“ Batch {i}/{len(batches)} complete")

                # Persist for future use
                self.persist_dir.mkdir(exist_ok=True)
                self.index.storage_context.persist(persist_dir=str(self.persist_dir))
                logger.info(f"Index saved to {self.persist_dir}")

            # Create query engine
            self.query_engine = self.index.as_query_engine(
                similarity_top_k=2,  # Reduced from 3 to save API calls
                response_mode="compact"
            )

            return True

        except FileNotFoundError as e:
            logger.error(f"Data directory or files not found: {e}")
            raise
        except ValueError as e:
            logger.error(f"Invalid data or configuration: {e}")
            raise
        except (ConnectionError, TimeoutError) as e:
            logger.error(f"API connection error during index creation: {e}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error initializing index: {e}", exc_info=True)
            raise

    def get_cited_answer(self, query: str, use_cache: bool = False, product_context: Optional[str] = None) -> Tuple[str, List[Dict]]:
        """
        Query with optional caching to avoid repeated API calls.

        Args:
            query: The query string
            use_cache: Whether to use cached results (default False for dynamic queries)
            product_context: Product description to make cache product-specific

        Returns:
            Tuple of (answer, citations)
        """
        if not self.query_engine:
            raise RuntimeError("Index not initialized")

        # Check cache first (with product context)
        query_hash = self._get_query_hash(query, product_context)
        if use_cache and query_hash in self.query_cache:
            logger.info("Cache hit: Using cached query result (no API call)")
            cached = self.query_cache[query_hash]
            return cached['answer'], cached['citations']

        try:
            # Make API call
            response = self.query_engine.query(query)
            answer = str(response)

            # Extract citations
            citations = []
            if hasattr(response, 'source_nodes'):
                for i, node in enumerate(response.source_nodes):
                    metadata = node.node.metadata if hasattr(node.node, 'metadata') else {}
                    file_name = Path(metadata.get('file_name', 'Unknown')).stem
                    page_num = metadata.get('page_label', 'N/A')
                    score = round(node.score, 3) if hasattr(node, 'score') else None
                    text_excerpt = node.node.text[:200] + "..."

                    citations.append({
                        "source_id": i + 1,
                        "file_name": file_name,
                        "page": page_num,
                        "relevance_score": score,
                        "excerpt": text_excerpt
                    })

            # Cache the result
            self.query_cache[query_hash] = {
                'answer': answer,
                'citations': citations
            }
            self._save_query_cache()

            return answer, citations

        except (ConnectionError, TimeoutError) as e:
            logger.error(f"API connection error during query: {e}")
            raise
        except ValueError as e:
            logger.error(f"Invalid query or response format: {e}")
            raise
        except Exception as e:
            logger.error(f"Unexpected error during query: {e}", exc_info=True)
            raise

    def query(self, query_str: str):
        """
        Query the index and return a response object.
        This method provides compatibility with different interfaces.

        Args:
            query_str: The question to ask

        Returns:
            Response object with .response attribute and .source_nodes
        """
        if not self.query_engine:
            raise RuntimeError("Index not initialized. Call initialize_index() first.")

        return self.query_engine.query(query_str)

    def get_citations(self, response) -> List[Dict]:
        """
        Extract citations from a response object.

        Args:
            response: Query response object

        Returns:
            List of citation dictionaries
        """
        citations = []
        if hasattr(response, 'source_nodes'):
            for i, node in enumerate(response.source_nodes):
                metadata = node.node.metadata if hasattr(node.node, 'metadata') else {}
                file_name = metadata.get('file_name', 'Unknown')
                if file_name != 'Unknown':
                    file_name = Path(file_name).stem

                citation = {
                    "source_id": i + 1,
                    "file_name": file_name,
                    "page": metadata.get('page_label', 'N/A'),
                    "relevance_score": round(node.score, 3) if hasattr(node, 'score') else None,
                    "excerpt": node.node.text[:200] + "..." if len(node.node.text) > 200 else node.node.text
                }
                citations.append(citation)
        return citations

    def get_marketing_strategy(
        self,
        product_concept: str,
        category: str,
        target_segment: str = "Skeptic",
        use_cache: bool = False
    ) -> Tuple[str, List[Dict]]:
        """
        Get marketing strategy with product-specific results.

        Args:
            product_concept: The product to analyze
            category: Product category
            target_segment: Target consumer segment
            use_cache: Whether to use cached results (default False for dynamic queries)
        """
        query = f"""Based on research about {category} and {target_segment} consumers:

Product: {product_concept}

Provide a concise marketing strategy (3-4 key points) addressing:
1. Key psychological factors for this segment
2. Recommended messaging approach
3. What to avoid

Cite specific research findings."""

        return self.get_cited_answer(query, use_cache=use_cache, product_context=product_concept)

    def get_segment_strategy(
        self,
        product_concept: str,
        target_segment: str,
        use_cache: bool = False
    ) -> Tuple[str, List[Dict]]:
        """
        Get segment-specific strategy without category filter.

        Args:
            product_concept: The product to analyze
            target_segment: Target consumer segment
            use_cache: Whether to use cached results (default False for dynamic queries)
        """
        query = f"""Based on research about {target_segment} consumers and sustainable food:

Product: {product_concept}

Provide a concise marketing strategy (3-4 key points) addressing:
1. Key psychological factors for this segment
2. Recommended messaging approach
3. What to avoid

Cite specific research findings."""

        return self.get_cited_answer(query, use_cache=use_cache, product_context=product_concept)

    def get_general_strategy(
        self,
        product_concept: str,
        category: str,
        use_cache: bool = False
    ) -> Tuple[str, List[Dict]]:
        """
        Get category-specific strategy for general audience.

        Args:
            product_concept: The product to analyze
            category: Product category
            use_cache: Whether to use cached results (default False for dynamic queries)
        """
        query = f"""Based on research about {category} products:

Product: {product_concept}

Provide a general marketing strategy (3-4 key points) addressing:
1. Key consumer acceptance factors
2. Recommended messaging approach
3. Common barriers to address

Cite specific research findings."""

        return self.get_cited_answer(query, use_cache=use_cache, product_context=product_concept)

    def get_universal_strategy(
        self,
        product_concept: str,
        use_cache: bool = False
    ) -> Tuple[str, List[Dict]]:
        """
        Get universal strategy for sustainable food products.

        Args:
            product_concept: The product to analyze
            use_cache: Whether to use cached results (default False for dynamic queries)
        """
        query = f"""Based on research about sustainable food alternatives:

Product: {product_concept}

Provide a universal marketing strategy (3-4 key points) addressing:
1. General consumer acceptance factors
2. Recommended messaging approach
3. Common barriers across all sustainable food categories

Cite specific research findings."""

        return self.get_cited_answer(query, use_cache=use_cache, product_context=product_concept)

    def get_consumer_insights(self, category: str, product_context: Optional[str] = None, use_cache: bool = False) -> Tuple[str, List[Dict]]:
        """
        Get consumer insights with optional product context.

        Args:
            category: Product category
            product_context: Optional product description for context
            use_cache: Whether to use cached results (default False for dynamic queries)
        """
        query = f"""Summarize key consumer acceptance factors for {category} products:
- Main barriers
- Success factors
- Role of familiarity"""

        if product_context:
            query += f"\n\nSpecific product context: {product_context}"

        return self.get_cited_answer(query, use_cache=use_cache, product_context=product_context)

    def clear_cache(self):
        """Clear query cache to force fresh API calls."""
        cache_size = len(self.query_cache)
        self.query_cache = {}
        self._save_query_cache()
        logger.info(f"Query cache cleared ({cache_size} entries removed)")
